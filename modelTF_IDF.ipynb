{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "folder_pickle= '../../data/pickle'\n",
    "folder_text= '../../data/text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data_fusion.csv')\n",
    "dataClean= data.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(174295, 19)\n",
      "(174295, 19)\n"
     ]
    }
   ],
   "source": [
    "print(dataClean.shape)\n",
    "print(dataClean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not os.path.exists(folder_pickle):\n",
    "    os.makedirs(folder_pickle, exist_ok=True)\n",
    "if not os.path.exists(folder_text):\n",
    "    os.makedirs(folder_text, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 6):\n",
    "    with open(os.path.join(folder_text, f\"{i}star.txt\"), 'w') as f:\n",
    "        f.write('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2999782/2999782 [12:37<00:00, 3962.39it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Parcourir le dataframe et ajouter le commentaire dans le fichier correspondant à la note\n",
    "for index, row in tqdm(data.iterrows(), total=data.shape[0]):\n",
    "    note = int(row['revue/score'])\n",
    "    commentaire = row['revue/texte']\n",
    "    if pd.notna(commentaire):  # Check if commentaire is not NaN\n",
    "        with open(os.path.join(folder_text, f\"{note}star.txt\"), 'a', encoding='utf-8') as f:\n",
    "            f.write(commentaire + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in c:\\users\\mathi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\mathi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\mathi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\mathi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\mathi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\mathi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "The strip bat be hang on their foot for best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: C:\\Users\\mathi\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\mathi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\mathi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mathi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mathi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\mathi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%pip install nltk\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Télécharger les ressources nécessaires\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = word_tokenize(text)\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in words]\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "# Exemple d'utilisation\n",
    "text = \"The striped bats are hanging on their feet for best\"\n",
    "lemmatized_text = lemmatize_text(text)\n",
    "print(lemmatized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 6):\n",
    "    input_file_path = os.path.join(folder_text, f\"{i}star.txt\")\n",
    "    output_file_path = os.path.join(folder_text, f\"{i}star_lemmatized.txt\")\n",
    "    \n",
    "    with open(input_file_path, 'r', encoding='utf-8') as input_file:\n",
    "        content = input_file.read()\n",
    "    \n",
    "    lemmatized_content = lemmatize_text(content)\n",
    "    \n",
    "    with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "        output_file.write(lemmatized_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features names: ['1star', '2star', '3star', '4star', '5star']\n",
      "TF-IDF Matrix:\n",
      " [[3.24114647e-04 9.14055282e-04 2.22759208e-06 ... 7.79142017e-07\n",
      "  7.79142017e-07 0.00000000e+00]\n",
      " [1.64630849e-04 6.71443792e-04 4.16786960e-07 ... 0.00000000e+00\n",
      "  0.00000000e+00 1.74934847e-06]\n",
      " [1.14628101e-04 5.72681074e-04 9.18862534e-07 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [7.94323596e-05 6.12585684e-04 3.11907171e-07 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [1.26408830e-04 6.37399027e-04 6.32465794e-07 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# Initialiser le vecteur TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Extraire les textes des fichiers lemmatisés\n",
    "texts = []\n",
    "for i in range(1, 6):\n",
    "    with open(os.path.join(folder_text, f\"{i}star_lemmatized.txt\"), 'r', encoding='utf-8') as file:\n",
    "        texts.append(file.read())\n",
    "\n",
    "# Ajuster et transformer les textes en une matrice TF-IDF\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(texts)\n",
    "\n",
    "# Renommer les features pour qu'elles correspondent aux fichiers\n",
    "feature_names = [f\"{i}star\" for i in range(1, 6)]\n",
    "\n",
    "# Afficher les termes et la matrice TF-IDF\n",
    "print(\"Features names:\", feature_names)\n",
    "print(\"TF-IDF Matrix:\\n\", tfidf_matrix.toarray())\n",
    "\n",
    "# Enregistrer la matrice TF-IDF dans un fichier pickle\n",
    "with open(os.path.join(folder_pickle, 'tfidf_matrix.pkl'), 'wb') as file:\n",
    "    pickle.dump(tfidf_matrix, file)\n",
    "    # Enregistrer le TF-IDF vectorizer dans un fichier pickle\n",
    "with open(os.path.join(folder_pickle, 'tfidf_vectorizer.pkl'), 'wb') as file:\n",
    "    pickle.dump(tfidf_vectorizer, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commentaire de test 1:\n",
      "Similarité cosinus la plus élevée avec 5star: 0.43549307023583755\n",
      "\n",
      "\n",
      "Commentaire de test 2:\n",
      "Similarité cosinus la plus élevée avec 4star: 0.44310138199965987\n",
      "\n",
      "\n",
      "Commentaire de test 3:\n",
      "Similarité cosinus la plus élevée avec 1star: 0.55323483680274\n",
      "\n",
      "\n",
      "Commentaire de test 4:\n",
      "Similarité cosinus la plus élevée avec 5star: 0.173957298511708\n",
      "\n",
      "\n",
      "Commentaire de test 5:\n",
      "Similarité cosinus la plus élevée avec 4star: 0.4526659202881725\n",
      "\n",
      "\n",
      "Commentaire de test 6:\n",
      "Similarité cosinus la plus élevée avec 5star: 0.3636150330947245\n",
      "\n",
      "\n",
      "Commentaire de test 7:\n",
      "Similarité cosinus la plus élevée avec 4star: 0.45732947824172954\n",
      "\n",
      "\n",
      "Commentaire de test 8:\n",
      "Similarité cosinus la plus élevée avec 4star: 0.44013352936201416\n",
      "\n",
      "\n",
      "Commentaire de test 9:\n",
      "Similarité cosinus la plus élevée avec 1star: 0.2609118878250282\n",
      "\n",
      "\n",
      "Commentaire de test 10:\n",
      "Similarité cosinus la plus élevée avec 5star: 0.4938654797819173\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "#\n",
    "# Utiliser la matrice TF-IDF pour faire un commentaire de test\n",
    "test_comments = [\n",
    "    \"This book provides a comprehensive overview of the subject and is very well written.\",\n",
    "    \"The storyline was captivating and the characters were well developed.\",\n",
    "    \"I found the book to be quite boring and not engaging at all.\",\n",
    "    \"An excellent read with lots of useful information.\",\n",
    "    \"The plot was predictable and the writing style was not to my liking.\",\n",
    "    \"The book was too long and filled with unnecessary details.\",\n",
    "    \"I didn't like the main character and found the plot confusing.\",\n",
    "    \"The writing was poor and the story didn't hold my interest.\",\n",
    "    \"I wouldn't recommend this book to anyone.\",\n",
    "    \"The book was a waste of time and money.\"\n",
    "]\n",
    "# Charger la matrice TF-IDF à partir du fichier pickle\n",
    "with open(os.path.join(folder_pickle, 'tfidf_matrix.pkl'), 'rb') as file:\n",
    "    tfidf_matrix = pickle.load(file)\n",
    "# Transformer le commentaire de test en vecteur TF-IDF\n",
    "with open(os.path.join(folder_pickle, 'tfidf_vectorizer.pkl'), 'rb') as file:\n",
    "    tfidf_vectorizer_tester = pickle.load(file)\n",
    "\n",
    "test_vectors = tfidf_vectorizer_tester.transform(test_comments)\n",
    "\n",
    "# Calculer la similarité cosinus entre les commentaires de test et les textes existants\n",
    "for idx, test_vector in enumerate(test_vectors):\n",
    "    similarities = cosine_similarity(test_vector, tfidf_matrix)\n",
    "    max_similarity_index = similarities.argmax()\n",
    "    print(f\"Commentaire de test {idx + 1}:\")\n",
    "    print(f\"Similarité cosinus la plus élevée avec {feature_names[max_similarity_index]}: {similarities[0][max_similarity_index]}\")\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
