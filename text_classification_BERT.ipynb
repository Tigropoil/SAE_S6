{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tigropoil/SAE_S6/blob/Arthur/text_classification_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import numpy as np\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "LOzRKNq3yF0w"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Charger les données\n",
        "data_url = '/content/drive/MyDrive/SAE S6/data_fusion_little.csv'\n",
        "data = pd.read_csv(data_url)"
      ],
      "metadata": {
        "id": "cWhYsl5TszOh"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "DmNZjmU0ptim"
      },
      "outputs": [],
      "source": [
        "# Sélection des colonnes pertinentes\n",
        "columns_to_keep = ['revue/texte', 'revue/score']\n",
        "data = data[columns_to_keep].dropna()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convertir \"revue/score\" en classe catégorielle\n",
        "num_labels = data['revue/score'].nunique()\n",
        "data['revue/score'] = data['revue/score'].astype(int)"
      ],
      "metadata": {
        "id": "iYtDAHrJyZxi"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split des données\n",
        "train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "max_seq_len = 128"
      ],
      "metadata": {
        "id": "Q65Yq8BBymZo"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_data(data, tokenizer, max_seq_len):\n",
        "    input_ids, attention_masks, labels = [], [], []\n",
        "\n",
        "    for _, row in tqdm(data.iterrows(), total=len(data)):\n",
        "        encoded = tokenizer.encode_plus(\n",
        "            row['revue/texte'],\n",
        "            add_special_tokens=True,\n",
        "            max_length=max_seq_len,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "        )\n",
        "\n",
        "        input_ids.append(encoded['input_ids'])\n",
        "        attention_masks.append(encoded['attention_mask'])\n",
        "        labels.append(row['revue/score'])\n",
        "\n",
        "    return torch.tensor(input_ids), torch.tensor(attention_masks), torch.tensor(labels)"
      ],
      "metadata": {
        "id": "Zxx6WVsGyqCH"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_input_ids, train_attention_masks, train_labels = tokenize_data(train_data, tokenizer, max_seq_len)\n",
        "val_input_ids, val_attention_masks, val_labels = tokenize_data(val_data, tokenizer, max_seq_len)"
      ],
      "metadata": {
        "id": "mQQntFO6yrrS",
        "outputId": "902067d8-d599-433c-aa5b-91605af4aa3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  8%|▊         | 22072/287952 [01:56<13:49, 320.48it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DataLoader\n",
        "batch_size = 16\n",
        "train_dataset = TensorDataset(train_input_ids, train_attention_masks, train_labels)\n",
        "val_dataset = TensorDataset(val_input_ids, val_attention_masks, val_labels)\n",
        "train_dataloader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=batch_size)\n",
        "val_dataloader = DataLoader(val_dataset, sampler=SequentialSampler(val_dataset), batch_size=batch_size)"
      ],
      "metadata": {
        "id": "PROs7oUvy1GN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Modèle DistilBERT pour classification multi-classe\n",
        "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=num_labels)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "xG80HuXDy2-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimizer et scheduler\n",
        "num_epochs = 3\n",
        "total_steps = len(train_dataloader) * num_epochs\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)"
      ],
      "metadata": {
        "id": "gy8Bp151y4zZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fonction d'entraînement\n",
        "def train_epoch(model, dataloader, optimizer, scheduler, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
        "        input_ids, attention_masks, labels = [t.to(device) for t in batch]\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids, attention_mask=attention_masks, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "    return total_loss / len(dataloader)"
      ],
      "metadata": {
        "id": "yVFqk46Ny7Ss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fonction d'évaluation\n",
        "def evaluate(model, dataloader, device):\n",
        "    model.eval()\n",
        "    predictions, true_labels = [], []\n",
        "\n",
        "    for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
        "        input_ids, attention_masks, labels = [t.to(device) for t in batch]\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids, attention_mask=attention_masks)\n",
        "        logits = outputs.logits.detach().cpu().numpy()\n",
        "        label_ids = labels.cpu().numpy()\n",
        "        predictions.extend(logits.argmax(axis=-1))\n",
        "        true_labels.extend(label_ids)\n",
        "\n",
        "    return accuracy_score(true_labels, predictions), classification_report(true_labels, predictions, digits=4)\n"
      ],
      "metadata": {
        "id": "j0rwvwgIy9vb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entraînement du modèle\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train_epoch(model, train_dataloader, optimizer, scheduler, device)\n",
        "    val_accuracy, report = evaluate(model, val_dataloader, device)\n",
        "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
        "    print(f\"Loss: {train_loss:.4f} - Validation Accuracy: {val_accuracy:.4f}\")\n",
        "    print(report)"
      ],
      "metadata": {
        "id": "WSxDrxfczAmP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sauvegarde du modèle\n",
        "model.save_pretrained(\"./model/\")\n",
        "tokenizer.save_pretrained(\"./model/\")\n"
      ],
      "metadata": {
        "id": "9wOglB0PzA8R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}