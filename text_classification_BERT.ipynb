{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tigropoil/SAE_S6/blob/Arthur/text_classification_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "O5TLTNmrsj-8",
        "outputId": "315cf2d9-22bc-486c-b20c-ae3ffa00721e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import numpy as np\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "LOzRKNq3yF0w"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Charger les données\n",
        "data_url = '/content/data_fusion_sample.csv'\n",
        "data = pd.read_csv(data_url)"
      ],
      "metadata": {
        "id": "cWhYsl5TszOh"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "DmNZjmU0ptim"
      },
      "outputs": [],
      "source": [
        "# Sélection des colonnes pertinentes\n",
        "columns_to_keep = ['revue/texte', 'revue/score']\n",
        "data = data[columns_to_keep].dropna()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convertir \"revue/score\" en classe catégorielle (recalage entre 0 et num_labels-1)\n",
        "data['revue/score'] = data['revue/score'].astype(int) - 1\n",
        "num_labels = data['revue/score'].nunique()"
      ],
      "metadata": {
        "id": "iYtDAHrJyZxi"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split des données\n",
        "train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "max_seq_len = 128"
      ],
      "metadata": {
        "id": "Q65Yq8BBymZo",
        "outputId": "9f48ce2a-c630-48c1-b63b-d577c0e3a5fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
            "The class this function is called from is 'DistilBertTokenizer'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_data(data, tokenizer, max_seq_len):\n",
        "    input_ids, attention_masks, labels = [], [], []\n",
        "\n",
        "    for _, row in tqdm(data.iterrows(), total=len(data)):\n",
        "        encoded = tokenizer.encode_plus(\n",
        "            row['revue/texte'],\n",
        "            add_special_tokens=True,\n",
        "            max_length=max_seq_len,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "        )\n",
        "\n",
        "        input_ids.append(encoded['input_ids'])\n",
        "        attention_masks.append(encoded['attention_mask'])\n",
        "        labels.append(row['revue/score'])\n",
        "\n",
        "    return torch.tensor(input_ids), torch.tensor(attention_masks), torch.tensor(labels)"
      ],
      "metadata": {
        "id": "Zxx6WVsGyqCH"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_input_ids, train_attention_masks, train_labels = tokenize_data(train_data, tokenizer, max_seq_len)\n",
        "val_input_ids, val_attention_masks, val_labels = tokenize_data(val_data, tokenizer, max_seq_len)"
      ],
      "metadata": {
        "id": "mQQntFO6yrrS",
        "outputId": "2ecd0b17-015c-4f1d-dee6-c852af586b2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1200/1200 [00:14<00:00, 80.92it/s] \n",
            "100%|██████████| 300/300 [00:02<00:00, 121.49it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DataLoader\n",
        "batch_size = 24\n",
        "train_dataset = TensorDataset(train_input_ids, train_attention_masks, train_labels)\n",
        "val_dataset = TensorDataset(val_input_ids, val_attention_masks, val_labels)\n",
        "train_dataloader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=batch_size)\n",
        "val_dataloader = DataLoader(val_dataset, sampler=SequentialSampler(val_dataset), batch_size=batch_size)"
      ],
      "metadata": {
        "id": "PROs7oUvy1GN"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Modèle DistilBERT pour classification multi-classe\n",
        "model = DistilBertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=num_labels)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "xG80HuXDy2-K",
        "outputId": "ff9b4f60-acc0-48c0-f5fa-895b94fd717a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using a model of type bert to instantiate a model of type distilbert. This is not supported for all configurations of models and can yield errors.\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'embeddings.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'embeddings.word_embeddings.weight', 'pre_classifier.bias', 'pre_classifier.weight', 'transformer.layer.0.attention.k_lin.bias', 'transformer.layer.0.attention.k_lin.weight', 'transformer.layer.0.attention.out_lin.bias', 'transformer.layer.0.attention.out_lin.weight', 'transformer.layer.0.attention.q_lin.bias', 'transformer.layer.0.attention.q_lin.weight', 'transformer.layer.0.attention.v_lin.bias', 'transformer.layer.0.attention.v_lin.weight', 'transformer.layer.0.ffn.lin1.bias', 'transformer.layer.0.ffn.lin1.weight', 'transformer.layer.0.ffn.lin2.bias', 'transformer.layer.0.ffn.lin2.weight', 'transformer.layer.0.output_layer_norm.bias', 'transformer.layer.0.output_layer_norm.weight', 'transformer.layer.0.sa_layer_norm.bias', 'transformer.layer.0.sa_layer_norm.weight', 'transformer.layer.1.attention.k_lin.bias', 'transformer.layer.1.attention.k_lin.weight', 'transformer.layer.1.attention.out_lin.bias', 'transformer.layer.1.attention.out_lin.weight', 'transformer.layer.1.attention.q_lin.bias', 'transformer.layer.1.attention.q_lin.weight', 'transformer.layer.1.attention.v_lin.bias', 'transformer.layer.1.attention.v_lin.weight', 'transformer.layer.1.ffn.lin1.bias', 'transformer.layer.1.ffn.lin1.weight', 'transformer.layer.1.ffn.lin2.bias', 'transformer.layer.1.ffn.lin2.weight', 'transformer.layer.1.output_layer_norm.bias', 'transformer.layer.1.output_layer_norm.weight', 'transformer.layer.1.sa_layer_norm.bias', 'transformer.layer.1.sa_layer_norm.weight', 'transformer.layer.10.attention.k_lin.bias', 'transformer.layer.10.attention.k_lin.weight', 'transformer.layer.10.attention.out_lin.bias', 'transformer.layer.10.attention.out_lin.weight', 'transformer.layer.10.attention.q_lin.bias', 'transformer.layer.10.attention.q_lin.weight', 'transformer.layer.10.attention.v_lin.bias', 'transformer.layer.10.attention.v_lin.weight', 'transformer.layer.10.ffn.lin1.bias', 'transformer.layer.10.ffn.lin1.weight', 'transformer.layer.10.ffn.lin2.bias', 'transformer.layer.10.ffn.lin2.weight', 'transformer.layer.10.output_layer_norm.bias', 'transformer.layer.10.output_layer_norm.weight', 'transformer.layer.10.sa_layer_norm.bias', 'transformer.layer.10.sa_layer_norm.weight', 'transformer.layer.11.attention.k_lin.bias', 'transformer.layer.11.attention.k_lin.weight', 'transformer.layer.11.attention.out_lin.bias', 'transformer.layer.11.attention.out_lin.weight', 'transformer.layer.11.attention.q_lin.bias', 'transformer.layer.11.attention.q_lin.weight', 'transformer.layer.11.attention.v_lin.bias', 'transformer.layer.11.attention.v_lin.weight', 'transformer.layer.11.ffn.lin1.bias', 'transformer.layer.11.ffn.lin1.weight', 'transformer.layer.11.ffn.lin2.bias', 'transformer.layer.11.ffn.lin2.weight', 'transformer.layer.11.output_layer_norm.bias', 'transformer.layer.11.output_layer_norm.weight', 'transformer.layer.11.sa_layer_norm.bias', 'transformer.layer.11.sa_layer_norm.weight', 'transformer.layer.2.attention.k_lin.bias', 'transformer.layer.2.attention.k_lin.weight', 'transformer.layer.2.attention.out_lin.bias', 'transformer.layer.2.attention.out_lin.weight', 'transformer.layer.2.attention.q_lin.bias', 'transformer.layer.2.attention.q_lin.weight', 'transformer.layer.2.attention.v_lin.bias', 'transformer.layer.2.attention.v_lin.weight', 'transformer.layer.2.ffn.lin1.bias', 'transformer.layer.2.ffn.lin1.weight', 'transformer.layer.2.ffn.lin2.bias', 'transformer.layer.2.ffn.lin2.weight', 'transformer.layer.2.output_layer_norm.bias', 'transformer.layer.2.output_layer_norm.weight', 'transformer.layer.2.sa_layer_norm.bias', 'transformer.layer.2.sa_layer_norm.weight', 'transformer.layer.3.attention.k_lin.bias', 'transformer.layer.3.attention.k_lin.weight', 'transformer.layer.3.attention.out_lin.bias', 'transformer.layer.3.attention.out_lin.weight', 'transformer.layer.3.attention.q_lin.bias', 'transformer.layer.3.attention.q_lin.weight', 'transformer.layer.3.attention.v_lin.bias', 'transformer.layer.3.attention.v_lin.weight', 'transformer.layer.3.ffn.lin1.bias', 'transformer.layer.3.ffn.lin1.weight', 'transformer.layer.3.ffn.lin2.bias', 'transformer.layer.3.ffn.lin2.weight', 'transformer.layer.3.output_layer_norm.bias', 'transformer.layer.3.output_layer_norm.weight', 'transformer.layer.3.sa_layer_norm.bias', 'transformer.layer.3.sa_layer_norm.weight', 'transformer.layer.4.attention.k_lin.bias', 'transformer.layer.4.attention.k_lin.weight', 'transformer.layer.4.attention.out_lin.bias', 'transformer.layer.4.attention.out_lin.weight', 'transformer.layer.4.attention.q_lin.bias', 'transformer.layer.4.attention.q_lin.weight', 'transformer.layer.4.attention.v_lin.bias', 'transformer.layer.4.attention.v_lin.weight', 'transformer.layer.4.ffn.lin1.bias', 'transformer.layer.4.ffn.lin1.weight', 'transformer.layer.4.ffn.lin2.bias', 'transformer.layer.4.ffn.lin2.weight', 'transformer.layer.4.output_layer_norm.bias', 'transformer.layer.4.output_layer_norm.weight', 'transformer.layer.4.sa_layer_norm.bias', 'transformer.layer.4.sa_layer_norm.weight', 'transformer.layer.5.attention.k_lin.bias', 'transformer.layer.5.attention.k_lin.weight', 'transformer.layer.5.attention.out_lin.bias', 'transformer.layer.5.attention.out_lin.weight', 'transformer.layer.5.attention.q_lin.bias', 'transformer.layer.5.attention.q_lin.weight', 'transformer.layer.5.attention.v_lin.bias', 'transformer.layer.5.attention.v_lin.weight', 'transformer.layer.5.ffn.lin1.bias', 'transformer.layer.5.ffn.lin1.weight', 'transformer.layer.5.ffn.lin2.bias', 'transformer.layer.5.ffn.lin2.weight', 'transformer.layer.5.output_layer_norm.bias', 'transformer.layer.5.output_layer_norm.weight', 'transformer.layer.5.sa_layer_norm.bias', 'transformer.layer.5.sa_layer_norm.weight', 'transformer.layer.6.attention.k_lin.bias', 'transformer.layer.6.attention.k_lin.weight', 'transformer.layer.6.attention.out_lin.bias', 'transformer.layer.6.attention.out_lin.weight', 'transformer.layer.6.attention.q_lin.bias', 'transformer.layer.6.attention.q_lin.weight', 'transformer.layer.6.attention.v_lin.bias', 'transformer.layer.6.attention.v_lin.weight', 'transformer.layer.6.ffn.lin1.bias', 'transformer.layer.6.ffn.lin1.weight', 'transformer.layer.6.ffn.lin2.bias', 'transformer.layer.6.ffn.lin2.weight', 'transformer.layer.6.output_layer_norm.bias', 'transformer.layer.6.output_layer_norm.weight', 'transformer.layer.6.sa_layer_norm.bias', 'transformer.layer.6.sa_layer_norm.weight', 'transformer.layer.7.attention.k_lin.bias', 'transformer.layer.7.attention.k_lin.weight', 'transformer.layer.7.attention.out_lin.bias', 'transformer.layer.7.attention.out_lin.weight', 'transformer.layer.7.attention.q_lin.bias', 'transformer.layer.7.attention.q_lin.weight', 'transformer.layer.7.attention.v_lin.bias', 'transformer.layer.7.attention.v_lin.weight', 'transformer.layer.7.ffn.lin1.bias', 'transformer.layer.7.ffn.lin1.weight', 'transformer.layer.7.ffn.lin2.bias', 'transformer.layer.7.ffn.lin2.weight', 'transformer.layer.7.output_layer_norm.bias', 'transformer.layer.7.output_layer_norm.weight', 'transformer.layer.7.sa_layer_norm.bias', 'transformer.layer.7.sa_layer_norm.weight', 'transformer.layer.8.attention.k_lin.bias', 'transformer.layer.8.attention.k_lin.weight', 'transformer.layer.8.attention.out_lin.bias', 'transformer.layer.8.attention.out_lin.weight', 'transformer.layer.8.attention.q_lin.bias', 'transformer.layer.8.attention.q_lin.weight', 'transformer.layer.8.attention.v_lin.bias', 'transformer.layer.8.attention.v_lin.weight', 'transformer.layer.8.ffn.lin1.bias', 'transformer.layer.8.ffn.lin1.weight', 'transformer.layer.8.ffn.lin2.bias', 'transformer.layer.8.ffn.lin2.weight', 'transformer.layer.8.output_layer_norm.bias', 'transformer.layer.8.output_layer_norm.weight', 'transformer.layer.8.sa_layer_norm.bias', 'transformer.layer.8.sa_layer_norm.weight', 'transformer.layer.9.attention.k_lin.bias', 'transformer.layer.9.attention.k_lin.weight', 'transformer.layer.9.attention.out_lin.bias', 'transformer.layer.9.attention.out_lin.weight', 'transformer.layer.9.attention.q_lin.bias', 'transformer.layer.9.attention.q_lin.weight', 'transformer.layer.9.attention.v_lin.bias', 'transformer.layer.9.attention.v_lin.weight', 'transformer.layer.9.ffn.lin1.bias', 'transformer.layer.9.ffn.lin1.weight', 'transformer.layer.9.ffn.lin2.bias', 'transformer.layer.9.ffn.lin2.weight', 'transformer.layer.9.output_layer_norm.bias', 'transformer.layer.9.output_layer_norm.weight', 'transformer.layer.9.sa_layer_norm.bias', 'transformer.layer.9.sa_layer_norm.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DistilBertForSequenceClassification(\n",
              "  (distilbert): DistilBertModel(\n",
              "    (embeddings): Embeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (transformer): Transformer(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x TransformerBlock(\n",
              "          (attention): DistilBertSdpaAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (activation): GELUActivation()\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
              "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
              "  (dropout): Dropout(p=0.2, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimizer et scheduler\n",
        "num_epochs = 20\n",
        "total_steps = len(train_dataloader) * num_epochs\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)"
      ],
      "metadata": {
        "id": "gy8Bp151y4zZ",
        "outputId": "7da3f67d-3dde-491d-89f8-0c0ed0f36df9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fonction d'entraînement\n",
        "def train_epoch(model, dataloader, optimizer, scheduler, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
        "        input_ids, attention_masks, labels = [t.to(device) for t in batch]\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids, attention_mask=attention_masks, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "    return total_loss / len(dataloader)"
      ],
      "metadata": {
        "id": "yVFqk46Ny7Ss"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fonction d'évaluation\n",
        "def evaluate(model, dataloader, device):\n",
        "    model.eval()\n",
        "    predictions, true_labels = [], []\n",
        "\n",
        "    for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
        "        input_ids, attention_masks, labels = [t.to(device) for t in batch]\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids, attention_mask=attention_masks)\n",
        "        logits = outputs.logits.detach().cpu().numpy()\n",
        "        label_ids = labels.cpu().numpy()\n",
        "        predictions.extend(logits.argmax(axis=-1))\n",
        "        true_labels.extend(label_ids)\n",
        "\n",
        "    return accuracy_score(true_labels, predictions), classification_report(true_labels, predictions, digits=4)\n"
      ],
      "metadata": {
        "id": "j0rwvwgIy9vb"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entraînement du modèle\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train_epoch(model, train_dataloader, optimizer, scheduler, device)\n",
        "    val_accuracy, report = evaluate(model, val_dataloader, device)\n",
        "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
        "    print(f\"Loss: {train_loss:.4f} - Validation Accuracy: {val_accuracy:.4f}\")\n",
        "    print(report)"
      ],
      "metadata": {
        "id": "WSxDrxfczAmP",
        "outputId": "16d6f2f7-a685-4c96-8643-c42ea4960e14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:23<00:00,  2.09it/s]\n",
            "Evaluating: 100%|██████████| 13/13 [00:02<00:00,  6.33it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/10\n",
            "Loss: 1.6229 - Validation Accuracy: 0.2000\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.1992    0.9107    0.3269        56\n",
            "           1     0.0000    0.0000    0.0000        65\n",
            "           2     0.2250    0.1837    0.2022        49\n",
            "           3     0.0000    0.0000    0.0000        66\n",
            "           4     0.0000    0.0000    0.0000        64\n",
            "\n",
            "    accuracy                         0.2000       300\n",
            "   macro avg     0.0848    0.2189    0.1058       300\n",
            "weighted avg     0.0739    0.2000    0.0941       300\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:24<00:00,  2.03it/s]\n",
            "Evaluating: 100%|██████████| 13/13 [00:02<00:00,  5.97it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2/10\n",
            "Loss: 1.6239 - Validation Accuracy: 0.1900\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.2143    0.2679    0.2381        56\n",
            "           1     0.0000    0.0000    0.0000        65\n",
            "           2     0.1826    0.8571    0.3011        49\n",
            "           3     0.0000    0.0000    0.0000        66\n",
            "           4     0.0000    0.0000    0.0000        64\n",
            "\n",
            "    accuracy                         0.1900       300\n",
            "   macro avg     0.0794    0.2250    0.1078       300\n",
            "weighted avg     0.0698    0.1900    0.0936       300\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:24<00:00,  2.04it/s]\n",
            "Evaluating: 100%|██████████| 13/13 [00:02<00:00,  6.36it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 3/10\n",
            "Loss: 1.6145 - Validation Accuracy: 0.1800\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     1.0000    0.0179    0.0351        56\n",
            "           1     0.0000    0.0000    0.0000        65\n",
            "           2     0.1713    1.0000    0.2925        49\n",
            "           3     0.2857    0.0303    0.0548        66\n",
            "           4     0.3333    0.0312    0.0571        64\n",
            "\n",
            "    accuracy                         0.1800       300\n",
            "   macro avg     0.3581    0.2159    0.0879       300\n",
            "weighted avg     0.3486    0.1800    0.0786       300\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:24<00:00,  2.06it/s]\n",
            "Evaluating: 100%|██████████| 13/13 [00:02<00:00,  6.35it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 4/10\n",
            "Loss: 1.6027 - Validation Accuracy: 0.1933\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.2667    0.1429    0.1860        56\n",
            "           1     0.0000    0.0000    0.0000        65\n",
            "           2     0.1815    0.9592    0.3052        49\n",
            "           3     0.0000    0.0000    0.0000        66\n",
            "           4     0.2727    0.0469    0.0800        64\n",
            "\n",
            "    accuracy                         0.1933       300\n",
            "   macro avg     0.1442    0.2298    0.1142       300\n",
            "weighted avg     0.1376    0.1933    0.1016       300\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:24<00:00,  2.05it/s]\n",
            "Evaluating: 100%|██████████| 13/13 [00:02<00:00,  6.28it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 5/10\n",
            "Loss: 1.5829 - Validation Accuracy: 0.2333\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.2308    0.3214    0.2687        56\n",
            "           1     0.2284    0.6923    0.3435        65\n",
            "           2     0.0000    0.0000    0.0000        49\n",
            "           3     0.0000    0.0000    0.0000        66\n",
            "           4     0.3684    0.1094    0.1687        64\n",
            "\n",
            "    accuracy                         0.2333       300\n",
            "   macro avg     0.1655    0.2246    0.1562       300\n",
            "weighted avg     0.1712    0.2333    0.1606       300\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:24<00:00,  2.05it/s]\n",
            "Evaluating: 100%|██████████| 13/13 [00:02<00:00,  6.37it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 6/10\n",
            "Loss: 1.5768 - Validation Accuracy: 0.2533\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.3182    0.3750    0.3443        56\n",
            "           1     0.0000    0.0000    0.0000        65\n",
            "           2     0.1847    0.5918    0.2816        49\n",
            "           3     0.0000    0.0000    0.0000        66\n",
            "           4     0.3611    0.4062    0.3824        64\n",
            "\n",
            "    accuracy                         0.2533       300\n",
            "   macro avg     0.1728    0.2746    0.2016       300\n",
            "weighted avg     0.1666    0.2533    0.1918       300\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:24<00:00,  2.06it/s]\n",
            "Evaluating: 100%|██████████| 13/13 [00:02<00:00,  6.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 7/10\n",
            "Loss: 1.5537 - Validation Accuracy: 0.2367\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.2812    0.1607    0.2045        56\n",
            "           1     0.2410    0.3077    0.2703        65\n",
            "           2     0.1746    0.4490    0.2514        49\n",
            "           3     0.0000    0.0000    0.0000        66\n",
            "           4     0.3636    0.3125    0.3361        64\n",
            "\n",
            "    accuracy                         0.2367       300\n",
            "   macro avg     0.2121    0.2460    0.2125       300\n",
            "weighted avg     0.2108    0.2367    0.2095       300\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:24<00:00,  2.05it/s]\n",
            "Evaluating: 100%|██████████| 13/13 [00:02<00:00,  6.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 8/10\n",
            "Loss: 1.5266 - Validation Accuracy: 0.2733\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.3585    0.3393    0.3486        56\n",
            "           1     0.0000    0.0000    0.0000        65\n",
            "           2     0.1825    0.5102    0.2688        49\n",
            "           3     0.4000    0.0909    0.1481        66\n",
            "           4     0.3404    0.5000    0.4051        64\n",
            "\n",
            "    accuracy                         0.2733       300\n",
            "   macro avg     0.2563    0.2881    0.2341       300\n",
            "weighted avg     0.2573    0.2733    0.2280       300\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:24<00:00,  2.05it/s]\n",
            "Evaluating: 100%|██████████| 13/13 [00:02<00:00,  6.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 9/10\n",
            "Loss: 1.5137 - Validation Accuracy: 0.2233\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.3095    0.2321    0.2653        56\n",
            "           1     0.0625    0.0308    0.0412        65\n",
            "           2     0.1688    0.5510    0.2584        49\n",
            "           3     0.2500    0.0152    0.0286        66\n",
            "           4     0.3871    0.3750    0.3810        64\n",
            "\n",
            "    accuracy                         0.2233       300\n",
            "   macro avg     0.2356    0.2408    0.1949       300\n",
            "weighted avg     0.2365    0.2233    0.1882       300\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:24<00:00,  2.06it/s]\n",
            "Evaluating: 100%|██████████| 13/13 [00:02<00:00,  6.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 10/10\n",
            "Loss: 1.4843 - Validation Accuracy: 0.2400\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.3088    0.3750    0.3387        56\n",
            "           1     0.0000    0.0000    0.0000        65\n",
            "           2     0.1812    0.5918    0.2775        49\n",
            "           3     0.4286    0.0455    0.0822        66\n",
            "           4     0.3393    0.2969    0.3167        64\n",
            "\n",
            "    accuracy                         0.2400       300\n",
            "   macro avg     0.2516    0.2618    0.2030       300\n",
            "weighted avg     0.2539    0.2400    0.1942       300\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sauvegarde du modèle\n",
        "model.save_pretrained(\"./model/\")\n",
        "tokenizer.save_pretrained(\"./model/\")\n"
      ],
      "metadata": {
        "id": "9wOglB0PzA8R",
        "outputId": "2df9e61c-f7e0-487f-bb93-dafc8bd1796f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./model/tokenizer_config.json',\n",
              " './model/special_tokens_map.json',\n",
              " './model/vocab.txt',\n",
              " './model/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fonction de prédiction\n",
        "def predict_review(review, model, tokenizer, device):\n",
        "    model.eval()\n",
        "    encoded = tokenizer.encode_plus(\n",
        "        review,\n",
        "        add_special_tokens=True,\n",
        "        max_length=128,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        return_attention_mask=True,\n",
        "    )\n",
        "    input_id = torch.tensor([encoded[\"input_ids\"]]).to(device)\n",
        "    attention_mask = torch.tensor([encoded[\"attention_mask\"]]).to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_id, attention_mask=attention_mask)\n",
        "    logits = outputs.logits.detach().cpu().numpy()\n",
        "    predicted_score = logits.argmax(axis=-1)[0] + 1  # Recaler le score pour correspondre à l'échelle originale\n",
        "    return predicted_score"
      ],
      "metadata": {
        "id": "SJxenZ4DY8k-"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report"
      ],
      "metadata": {
        "id": "9hHILjEwY-Ev"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Charger le modèle et le tokenizer\n",
        "model_path = \"model\"\n",
        "model = DistilBertForSequenceClassification.from_pretrained(model_path)\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(model_path)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "PW6Az_2WSdRd",
        "outputId": "362e3867-b57b-4520-8d02-e901f24bddd4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DistilBertForSequenceClassification(\n",
              "  (distilbert): DistilBertModel(\n",
              "    (embeddings): Embeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (transformer): Transformer(\n",
              "      (layer): ModuleList(\n",
              "        (0-5): 6 x TransformerBlock(\n",
              "          (attention): DistilBertSdpaAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (activation): GELUActivation()\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
              "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
              "  (dropout): Dropout(p=0.2, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fonction de prédiction\n",
        "def predict_reviews(df, model, tokenizer, device):\n",
        "    model.eval()\n",
        "    predicted_scores = []\n",
        "\n",
        "    for review in df['revue/texte']:\n",
        "        encoded = tokenizer.encode_plus(\n",
        "            review,\n",
        "            add_special_tokens=True,\n",
        "            max_length=128,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "        )\n",
        "        input_id = torch.tensor([encoded[\"input_ids\"]]).to(device)\n",
        "        attention_mask = torch.tensor([encoded[\"attention_mask\"]]).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_id, attention_mask=attention_mask)\n",
        "        logits = outputs.logits.detach().cpu().numpy()\n",
        "        predicted_score = logits.argmax(axis=-1)[0] + 1  # Recaler le score original\n",
        "        predicted_scores.append(predicted_score)\n",
        "\n",
        "    return predicted_scores"
      ],
      "metadata": {
        "id": "dMWxjaE-ShCE"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Évaluation\n",
        "def evaluate_model(df, model, tokenizer, device):\n",
        "    df = df[['revue/texte', 'revue/score']].dropna()\n",
        "    df['revue/score'] = df['revue/score'].astype(int)\n",
        "    df['predicted_score'] = predict_reviews(df, model, tokenizer, device)\n",
        "\n",
        "    # Calcul des métriques\n",
        "    cm = confusion_matrix(df['revue/score'], df['predicted_score'])\n",
        "    report = classification_report(df['revue/score'], df['predicted_score'], digits=4)\n",
        "\n",
        "    return cm, report, df"
      ],
      "metadata": {
        "id": "lof6uSpaSnub"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import du jeu de données\n",
        "data_url = '/content/drive/MyDrive/SAE S6/data_fusion_little.csv'\n",
        "df = pd.read_csv(data_url)\n",
        "\n",
        "# Ne garder que les colonnes nécessaires\n",
        "df = df[['revue/texte', 'revue/score']]\n",
        "df = df.dropna()\n",
        "\n",
        "# Ne garder que 100 lignes au hasard\n",
        "df = df.sample(n=10000, random_state=42)\n",
        "\n",
        "df_test = pd.DataFrame(df)\n",
        "\n"
      ],
      "metadata": {
        "id": "7n3xsTfJSpvn"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conf_matrix, class_report, df_results = evaluate_model(df_test, model, tokenizer, device)\n",
        "\n",
        "# Affichage des résultats\n",
        "print(\"Matrice de confusion :\\n\", conf_matrix)\n",
        "print(\"Rapport de classification :\\n\", class_report)"
      ],
      "metadata": {
        "id": "5qU6qTI8Sytz",
        "outputId": "31852bad-e3da-4593-bcbc-fde3714dca3d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrice de confusion :\n",
            " [[ 358   86   43   12   17]\n",
            " [ 128  107   85   33   21]\n",
            " [ 108  158  227  129  122]\n",
            " [  90  119  433  505  997]\n",
            " [ 204  151  556  792 4519]]\n",
            "Rapport de classification :\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           1     0.4032    0.6938    0.5100       516\n",
            "           2     0.1723    0.2861    0.2151       374\n",
            "           3     0.1689    0.3051    0.2174       744\n",
            "           4     0.3433    0.2355    0.2794      2144\n",
            "           5     0.7962    0.7263    0.7596      6222\n",
            "\n",
            "    accuracy                         0.5716     10000\n",
            "   macro avg     0.3768    0.4494    0.3963     10000\n",
            "weighted avg     0.6088    0.5716    0.5831     10000\n",
            "\n"
          ]
        }
      ]
    }
  ]
}